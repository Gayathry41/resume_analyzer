{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from datasets import Dataset, Sequence, ClassLabel\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Label Definitions\n",
    "# -------------------------\n",
    "labels_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Read Multi-file CoNLL\n",
    "# -------------------------\n",
    "def read_multiple_conll_files(file_list):\n",
    "    all_tokens, all_labels = [], []\n",
    "    for file_path in file_list:\n",
    "        tokens, labels = [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if tokens:\n",
    "                        all_tokens.append(tokens)\n",
    "                        all_labels.append(labels)\n",
    "                        tokens, labels = [], []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        token, tag = parts[0], parts[1]\n",
    "                        tokens.append(token)\n",
    "                        labels.append(tag)\n",
    "        if tokens:\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "    return {\"tokens\": all_tokens, \"ner_tags\": all_labels}\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Load Data from Files\n",
    "# -------------------------\n",
    "train_files = glob(\"data/train/*.txt\")\n",
    "val_files = glob(\"data/val/*.txt\")\n",
    "\n",
    "train_data = read_multiple_conll_files(train_files)\n",
    "val_data = read_multiple_conll_files(val_files)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Map tags to IDs\n",
    "# -------------------------\n",
    "train_dataset = train_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Define Features\n",
    "# -------------------------\n",
    "features = {\n",
    "    \"tokens\": train_dataset.features[\"tokens\"],\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=labels_list))\n",
    "}\n",
    "train_dataset = train_dataset.cast(features)\n",
    "val_dataset = val_dataset.cast(features)\n",
    "\n",
    "# -------------------------\n",
    "# Step 6: Tokenization & Label Alignment\n",
    "# -------------------------\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            current_label = example[\"ner_tags\"][word_idx]\n",
    "            label_name = labels_list[current_label]\n",
    "            labels.append(label2id[\"I-SKILL\"] if label_name.startswith(\"B\") else current_label)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# -------------------------\n",
    "# Step 7: Load Model\n",
    "# -------------------------\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(labels_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Step 8: Metrics\n",
    "# -------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[labels_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[labels_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
    "        \"f1\": report[\"micro avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Step 9: Trainer Setup\n",
    "# -------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Step 10: Train\n",
    "# -------------------------\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a910c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inference pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "text = \"We are hiring a Python developer with experience in Docker and machine learning.\"\n",
    "\n",
    "result = nlp(text)\n",
    "\n",
    "# Reconstruct full skills from B-SKILL/I-SKILL tokens\n",
    "skills = []\n",
    "current_skill = []\n",
    "\n",
    "for ent in result:\n",
    "    if ent[\"entity\"] == \"B-SKILL\":\n",
    "        if current_skill:\n",
    "            skills.append(\" \".join(current_skill))\n",
    "            current_skill = []\n",
    "        current_skill = [ent[\"word\"]]\n",
    "    elif ent[\"entity\"] == \"I-SKILL\":\n",
    "        current_skill.append(ent[\"word\"])\n",
    "    else:\n",
    "        if current_skill:\n",
    "            skills.append(\" \".join(current_skill))\n",
    "            current_skill = []\n",
    "\n",
    "if current_skill:\n",
    "    skills.append(\" \".join(current_skill))\n",
    "\n",
    "print(\"Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4a94",
   "metadata": {},
   "source": [
    "### PREPROCESSING\n",
    "- DETECTING SAME SKILL SEPTERATED BY SPACE (PRESENT IN SAME LINE AND REPLACING IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7060cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking file: ../data/val/test.txt\n",
      "\n",
      "[Line 2292] ‚ùå Malformed: New Relic B-SKILL (3 parts)\n"
     ]
    }
   ],
   "source": [
    "def find_malformed_lines(filepath):\n",
    "    print(f\"\\nüîç Checking file: {filepath}\\n\")\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            stripped = line.strip()\n",
    "            if stripped == \"\":\n",
    "                continue\n",
    "            parts = stripped.split()\n",
    "            if len(parts) > 2:\n",
    "                print(f\"[Line {i}] ‚ùå Malformed: {stripped} ({len(parts)} parts)\")\n",
    "\n",
    "# Example usage\n",
    "find_malformed_lines(\"../data/val/test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86cdc621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fixed file saved to: ../data/val/test.txt\n"
     ]
    }
   ],
   "source": [
    "def fix_malformed_lines(input_path, output_path):\n",
    "    fixed_lines = []\n",
    "    \n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                fixed_lines.append(\"\")  # Keep sentence breaks\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                fixed_lines.append(line)  # Proper line\n",
    "            elif len(parts) > 2:\n",
    "                # Assume last token is the label, rest is the entity\n",
    "                *tokens, label = parts\n",
    "                if label.startswith(\"B-\") or label.startswith(\"I-\"):\n",
    "                    fixed_lines.append(f\"{tokens[0]} {label}\")  # First word = B-label\n",
    "                    for token in tokens[1:]:\n",
    "                        fixed_lines.append(f\"{token} I{label[1:]}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Unknown label format: {line}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid line: {line}\")\n",
    "    \n",
    "    # Write output\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for line in fixed_lines:\n",
    "            f_out.write(line + \"\\n\")\n",
    "\n",
    "    print(f\"‚úÖ Fixed file saved to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "fix_malformed_lines(\"../data/val/test.txt\", \"../data/val/test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05fbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5ace6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d814b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733700a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b530769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f733d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95368f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, Sequence, ClassLabel,Features,Value\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseqeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from datasets import Dataset, Sequence, ClassLabel,Features,Value\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Label Definitions\n",
    "# -------------------------\n",
    "labels_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Read Multi-file CoNLL\n",
    "# -------------------------\n",
    "def read_multiple_conll_files(file_list):\n",
    "    all_tokens, all_labels = [], []\n",
    "    for file_path in file_list:\n",
    "        tokens, labels = [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if tokens and labels:\n",
    "                        all_tokens.append(tokens)\n",
    "                        all_labels.append(labels)\n",
    "                        tokens, labels = [], []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        token, tag = parts[0], parts[1]\n",
    "                        tokens.append(token)\n",
    "                        labels.append(tag)\n",
    "        if tokens and labels:\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "    return {\"tokens\": all_tokens, \"ner_tags\": all_labels}\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Load Data from Files\n",
    "# -------------------------\n",
    "train_files = glob(\"../data/train/*.txt\")\n",
    "val_files = glob(\"../data/val/*.txt\")\n",
    "\n",
    "train_data = read_multiple_conll_files(train_files)\n",
    "val_data = read_multiple_conll_files(val_files)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Map tags to IDs\n",
    "# -------------------------\n",
    "train_dataset = train_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Define Features\n",
    "# -------------------------\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=labels_list))\n",
    "})\n",
    "train_dataset = train_dataset.cast(features)\n",
    "val_dataset = val_dataset.cast(features)\n",
    "\n",
    "# -------------------------\n",
    "# Step 6: Tokenization & Label Alignment\n",
    "# -------------------------\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            current_label = example[\"ner_tags\"][word_idx]\n",
    "            label_name = labels_list[current_label]\n",
    "            if label_name.startswith(\"B\"):\n",
    "                labels.append(label2id[\"I-SKILL\"])\n",
    "            else:\n",
    "                labels.append(current_label)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# -------------------------\n",
    "# Step 7: Load Model\n",
    "# -------------------------\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(labels_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Step 8: Metrics\n",
    "# -------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[labels_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[labels_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": report[\"overall_precision\"],\n",
    "        \"recall\": report[\"overall_recall\"],\n",
    "        \"f1\": report[\"overall_f1\"]\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Step 9: Trainer Setup\n",
    "# -------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # <-- change from \"no\" to \"epoch\"\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disable WANDB/logging if not using\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbdf467",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade1348f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8fc0b",
   "metadata": {},
   "source": [
    "# TRAINING PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5e6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434376a63d0343d4856fd00f1e74a998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a821478a43455bb77b5793c89cd87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43649bbcdc6547ebaeaa5dcdbdfb86bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b23488f5f4682aae6ff5d534308de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a635c6dabc4f1087b2492397f6e639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c31a5a6a8540438340111dc5296009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4626f5a73e42d4bff177ba132b87fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashik/codes/latex_codes/venv310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6442, 'grad_norm': 3.0621626377105713, 'learning_rate': 4.5370370370370374e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3175, 'grad_norm': 3.0876271724700928, 'learning_rate': 4.074074074074074e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3463, 'grad_norm': 1.9984157085418701, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd60c384d649a4862160f9b9612961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.301613986492157, 'eval_precision': 0.7548387096774194, 'eval_recall': 0.6381818181818182, 'eval_f1': 0.6916256157635469, 'eval_runtime': 8.4118, 'eval_samples_per_second': 1.783, 'eval_steps_per_second': 0.238, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashik/codes/latex_codes/venv310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2143, 'grad_norm': 0.6205812692642212, 'learning_rate': 3.148148148148148e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2582, 'grad_norm': 3.0448570251464844, 'learning_rate': 2.6851851851851855e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2294, 'grad_norm': 1.6237772703170776, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n",
      "{'loss': 0.2165, 'grad_norm': 0.7305532693862915, 'learning_rate': 1.7592592592592595e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763fa24638fa48ef9b960be8a84ffc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2774098515510559, 'eval_precision': 0.728952772073922, 'eval_recall': 0.6454545454545455, 'eval_f1': 0.6846673095467696, 'eval_runtime': 8.1776, 'eval_samples_per_second': 1.834, 'eval_steps_per_second': 0.245, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashik/codes/latex_codes/venv310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1705, 'grad_norm': 2.279723882675171, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1779, 'grad_norm': 1.3308075666427612, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n",
      "{'loss': 0.161, 'grad_norm': 1.6121634244918823, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2a9fcbcf224ecc9256da16da71f9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3401213586330414, 'eval_precision': 0.7850467289719626, 'eval_recall': 0.610909090909091, 'eval_f1': 0.6871165644171779, 'eval_runtime': 9.5508, 'eval_samples_per_second': 1.571, 'eval_steps_per_second': 0.209, 'epoch': 3.0}\n",
      "{'train_runtime': 1951.0848, 'train_samples_per_second': 0.441, 'train_steps_per_second': 0.055, 'train_loss': 0.2666846403369197, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./ner_model/tokenizer_config.json',\n",
       " './ner_model/special_tokens_map.json',\n",
       " './ner_model/vocab.txt',\n",
       " './ner_model/added_tokens.json',\n",
       " './ner_model/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from datasets import Dataset, Sequence, ClassLabel,Features,Value\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Label Definitions\n",
    "# -------------------------\n",
    "labels_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Read Multi-file CoNLL\n",
    "# -------------------------\n",
    "def read_multiple_conll_files(file_list):\n",
    "    all_tokens, all_labels = [], []\n",
    "    for file_path in file_list:\n",
    "        tokens, labels = [], []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if tokens and labels:\n",
    "                        all_tokens.append(tokens)\n",
    "                        all_labels.append(labels)\n",
    "                        tokens, labels = [], []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        token, tag = parts[0], parts[1]\n",
    "                        tokens.append(token)\n",
    "                        labels.append(tag)\n",
    "        if tokens and labels:\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "    return {\"tokens\": all_tokens, \"ner_tags\": all_labels}\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Load Data from Files\n",
    "# -------------------------\n",
    "train_files = glob(\"../data/train/*.txt\")\n",
    "val_files = glob(\"../data/val/*.txt\")\n",
    "\n",
    "train_data = read_multiple_conll_files(train_files)\n",
    "val_data = read_multiple_conll_files(val_files)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Map tags to IDs\n",
    "# -------------------------\n",
    "train_dataset = train_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"ner_tags\": [label2id[tag] for tag in x[\"ner_tags\"]]})\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Define Features\n",
    "# -------------------------\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=labels_list))\n",
    "})\n",
    "train_dataset = train_dataset.cast(features)\n",
    "val_dataset = val_dataset.cast(features)\n",
    "\n",
    "# -------------------------\n",
    "# Step 6: Tokenization & Label Alignment\n",
    "# -------------------------\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True  # ‚úÖ Add this line\n",
    "    )\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            current_label = example[\"ner_tags\"][word_idx]\n",
    "            label_name = labels_list[current_label]\n",
    "            if label_name.startswith(\"B\"):\n",
    "                labels.append(label2id[\"I-SKILL\"])\n",
    "            else:\n",
    "                labels.append(current_label)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# -------------------------\n",
    "# Step 7: Load Model\n",
    "# -------------------------\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(labels_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Step 8: Metrics\n",
    "# -------------------------\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[labels_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_preds = [[labels_list[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]\n",
    "\n",
    "    report = classification_report(true_labels, true_preds, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1\": report[\"macro avg\"][\"f1-score\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Step 9: Trainer Setup\n",
    "# -------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # <-- change from \"no\" to \"epoch\"\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disable WANDB/logging if not using\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# Save model and tokenizer\n",
    "save_dir = \"./ner_model\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dfa26",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba62fdd",
   "metadata": {},
   "source": [
    "# LOAD MODEL AND TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cf95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(\"./ner_model\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./ner_model\")\n",
    "model.eval()  # Set to eval mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5bb19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7750d7ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16e5e8a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df30ed82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlooking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeveloper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mpredict_tokens\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m final_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m previous_word_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, word_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m word_idx \u001b[38;5;241m==\u001b[39m previous_word_idx:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "labels_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "\n",
    "def predict_tokens(tokens):\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "    word_ids = inputs.word_ids()[0]\n",
    "\n",
    "    final_preds = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        label_id = predictions[idx]\n",
    "        final_preds.append((tokens[word_idx], labels_list[label_id]))\n",
    "        previous_word_idx = word_idx\n",
    "    return final_preds\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"We\", \"are\", \"looking\", \"for\", \"a\", \"Python\", \"developer\", \".\"]\n",
    "print(predict_tokens(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f66850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_tokens(tokens):\n",
    "    # Tokenize without converting to tensor yet\n",
    "    encoding = tokenizer(tokens, is_split_into_words=True, truncation=True, return_offsets_mapping=True)\n",
    "    word_ids = encoding.word_ids()\n",
    "\n",
    "    # Now get torch tensor input\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "    final_preds = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        label_id = predictions[idx]\n",
    "        final_preds.append((tokens[word_idx], labels_list[label_id]))\n",
    "        previous_word_idx = word_idx\n",
    "    return final_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d3c054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'O'), ('are', 'O'), ('looking', 'O'), ('for', 'O'), ('a', 'O'), ('Python', 'B-SKILL'), ('developer', 'I-SKILL'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels_list = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "\n",
    "def predict_tokens(tokens):\n",
    "    # First, tokenize with return_offsets_mapping to access word_ids\n",
    "    encoding = tokenizer(tokens, is_split_into_words=True, return_offsets_mapping=True, truncation=True)\n",
    "    word_ids = encoding.word_ids()\n",
    "\n",
    "    # Prepare model input\n",
    "    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label IDs\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "    # Align predictions to input tokens\n",
    "    final_preds = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        label_id = predictions[idx]\n",
    "        final_preds.append((tokens[word_idx], labels_list[label_id]))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return final_preds\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "tokens = [\"We\", \"are\", \"looking\", \"for\", \"a\", \"Python\", \"developer\", \".\"]\n",
    "print(predict_tokens(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08328e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw predictions: [('We', 'O'), ('require', 'O'), ('experience', 'O'), ('in', 'O'), ('Java', 'B-SKILL'), ('programming', 'I-SKILL'), ('and', 'O'), ('machine', 'B-SKILL'), ('learning', 'I-SKILL'), ('frameworks', 'I-SKILL'), ('.', 'O')]\n",
      "‚úÖ Extracted skills: ['Java programming', 'machine learning frameworks']\n"
     ]
    }
   ],
   "source": [
    "def extract_skills(predictions):\n",
    "    skills = []\n",
    "    current_skill = []\n",
    "\n",
    "    for token, label in predictions:\n",
    "        if label == \"B-SKILL\":\n",
    "            if current_skill:\n",
    "                skills.append(\" \".join(current_skill))\n",
    "                current_skill = []\n",
    "            current_skill.append(token)\n",
    "        elif label == \"I-SKILL\":\n",
    "            if current_skill:\n",
    "                current_skill.append(token)\n",
    "        else:\n",
    "            if current_skill:\n",
    "                skills.append(\" \".join(current_skill))\n",
    "                current_skill = []\n",
    "\n",
    "    # Catch last skill if it ended the sentence\n",
    "    if current_skill:\n",
    "        skills.append(\" \".join(current_skill))\n",
    "\n",
    "    return skills\n",
    "\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "tokens = [\"We\", \"require\", \"experience\", \"in\", \"Java\", \"programming\", \"and\", \"machine\", \"learning\", \"frameworks\", \".\"]\n",
    "\n",
    "predictions = predict_tokens(tokens)\n",
    "print(\"üîç Raw predictions:\", predictions)\n",
    "\n",
    "skills = extract_skills(predictions)\n",
    "print(\"‚úÖ Extracted skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec1abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw Predictions: [('We', 'O'), ('are', 'O'), ('looking', 'O'), ('for', 'O'), ('a', 'O'), ('candidate', 'O'), ('with', 'O'), ('experience', 'O'), ('in', 'O'), ('Python', 'B-SKILL'), ('programming', 'I-SKILL'), (',', 'O'), ('machine', 'B-SKILL'), ('learning', 'I-SKILL'), (',', 'O'), ('and', 'O'), ('deep', 'B-SKILL'), ('learning', 'I-SKILL'), ('framework', 'I-SKILL'), ('like', 'O'), ('Ten', 'B-SKILL'), ('and', 'O'), ('P', 'B-SKILL'), ('.', 'O')]\n",
      "‚úÖ Extracted Skills: ['Python programming', 'machine learning', 'deep learning framework', 'Ten', 'P']\n"
     ]
    }
   ],
   "source": [
    "def predict_tokens_from_text(text):\n",
    "    # Tokenize with offset mapping (but don't pass it to the model)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    word_ids = tokenized.word_ids(0)  # works correctly\n",
    "\n",
    "    # Remove offset_mapping before passing to the model\n",
    "    inputs = {k: v for k, v in tokenized.items() if k != \"offset_mapping\"}\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    result = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or inputs[\"input_ids\"][0][idx] in tokenizer.all_special_ids:\n",
    "            continue  # Skip [CLS], [SEP], etc.\n",
    "        if word_idx != previous_word_idx:\n",
    "            token = tokens[idx]\n",
    "            label_id = predictions[idx]\n",
    "            result.append((token, id2label[label_id]))\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "    return result\n",
    "job_description = \"\"\"\n",
    "We are looking for a candidate with experience in Python programming, machine learning, and deep learning frameworks like TensorFlow and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_tokens_from_text(job_description)\n",
    "print(\"üîç Raw Predictions:\", predictions)\n",
    "\n",
    "skills = extract_skills(predictions)\n",
    "print(\"‚úÖ Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731773e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f6045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8337655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tokens_from_text(text):\n",
    "    # Split text into words\n",
    "    words = text.strip().split()\n",
    "\n",
    "    # Tokenize with alignment\n",
    "    tokenized = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    word_ids = tokenized.word_ids(0)\n",
    "\n",
    "    # Run prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)[0].tolist()\n",
    "\n",
    "    # Map predictions to words (not subwords)\n",
    "    result = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        word = words[word_idx]\n",
    "        label_id = predictions[idx]\n",
    "        result.append((word, id2label[label_id]))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65a1225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Raw Predictions: [('We', 'O'), ('are', 'O'), ('looking', 'O'), ('for', 'O'), ('a', 'O'), ('candidate', 'O'), ('with', 'O'), ('experience', 'O'), ('in', 'O'), ('Python', 'B-SKILL'), ('programming,', 'I-SKILL'), ('machine', 'B-SKILL'), ('learning,', 'I-SKILL'), ('and', 'O'), ('deep', 'B-SKILL'), ('learning', 'I-SKILL'), ('frameworks', 'I-SKILL'), ('like', 'O'), ('TensorFlow', 'B-SKILL'), ('and', 'O'), ('PyTorch.', 'B-SKILL')]\n",
      "‚úÖ Extracted Skills: ['Python programming,', 'machine learning,', 'deep learning frameworks', 'TensorFlow', 'PyTorch.']\n"
     ]
    }
   ],
   "source": [
    "job_description = \"\"\"\n",
    "We are looking for a candidate with experience in Python programming, machine learning, and deep learning frameworks like TensorFlow and PyTorch.\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_tokens_from_text(job_description)\n",
    "skills = extract_skills(predictions)\n",
    "\n",
    "print(\"üîç Raw Predictions:\", predictions)\n",
    "print(\"‚úÖ Extracted Skills:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64c5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310 (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
